{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb8fc9b4-2033-4924-b4e4-852490bb219e",
   "metadata": {},
   "source": [
    "## Machine Learning in Systems Security: Initial Assessment of LLM effectiveness against Social Engineering Attacks\n",
    "\n",
    "The following notebook walks through an initial exploration of various Large Language Models (LLMs) on detecting whether malicious information is solicited from LLM-generated chats that simulate normal human interaction. In the following code, we test the following models with a zero-shot approach:\n",
    "\n",
    "- Llama-3.1-8B-Instruct\n",
    "- Gemini 2.0 Flash\n",
    "- GPT-4\n",
    "\n",
    "### Prerequisites:\n",
    "\n",
    "If you have not used pip install to install any of the imported packages below, it would be a good idea to do so. Use the following example as a model of how to write pip install queries:\n",
    "\n",
    "```!pip install openai```\n",
    "\n",
    "### What are the goals of this project?\n",
    "- Can we do MORE, with a less pipelined approach? \n",
    "- Chain-of-Thought is a prompting strategy that can help the model \"think\" through whether a conversation is considered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7343a7-11cf-405b-b129-65d18b3ab363",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "\n",
    "# Base and Modelling Packages:\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig\n",
    "from huggingface_hub import login\n",
    "\n",
    "# LLM response-formatting packages (Langchain):\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "from typing import Optional\n",
    "\n",
    "# Miscellaneous:\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Import helper methods:\n",
    "from helpers import *\n",
    "\n",
    "with open('data/annotated_test.json') as f:\n",
    "    content = f.read()\n",
    "    data = json.loads(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc07476-be96-4cca-b41b-79e204f2b607",
   "metadata": {},
   "source": [
    "### Initial Step:\n",
    "\n",
    "First we will need to preprocess the test data provided through the .json files. The .json files are located in the data folder. See helpers.py for more details on how the data is preprocessed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a908f390-a3cc-4944-9bc6-38b3f6a68556",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "msgs, ground_truth = preprocess_test_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600ba404-8228-40a2-9630-1c1518e7e7fc",
   "metadata": {},
   "source": [
    "### Llama 3.1 Model Testing:\n",
    "\n",
    "The following code will test Llama 3.1 to see whether the LLM can accurately detect whether malicious information is solicited through a zero-shot approach. The prompt is noted in the helper file but as a reference we put it here:\n",
    "\n",
    "```The following is an interaction between two people. Determine whether malicious information is solicited from the conversation chain. Reply with yes or no. <|eot_id|> <|start_header_id|> {convo} <|end_header_id|```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8226f0f-86c0-428b-bd48-4ec5230fadfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the model and tokenizer for Llama 3.1-8B:\n",
    "\n",
    "TOKEN = ''\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, token=TOKEN, torch_dtype=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=TOKEN)\n",
    "\n",
    "# Add EOS tag to prevent Llama 3.1 from producing open-ended text generation:\n",
    "generation_config = model.generation_config\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4314cb13-56b4-460d-bc93-6ee3eb303704",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the Transformers Pipeline and read the ground truth dataset data into a DataFrame:\n",
    "pipeline = transformers.pipeline(\"text-generation\", model=model, torch_dtype=torch.float16, tokenizer = tokenizer, device='cuda')\n",
    "llama_llm_results = get_llama_results(pipeline, msgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc6559b-48d1-423a-be78-c6b2cc4d46ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract the LLM results:\n",
    "\n",
    "filtered_results = [re.search(r'<\\|end_header_id\\|>.*\\n\\n(.*)', entry).group(1).strip() for entry in llama_llm_results]\n",
    "filtered_results = [True if re.search(r'[Y|y]es', entry) else False for entry in filtered_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b91000b-b6bd-4a6d-a6f2-00d8af67fb68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Llama-3-8B Results: {get_accuracy(filtered_results, ground_truth)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348e738f-696f-4f3e-bee0-7206656a30cb",
   "metadata": {},
   "source": [
    "### Google Gemini 2.0 Flash:\n",
    "\n",
    "We will perform the same zero-shot prompting strategy with Google Gemini's model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b9b29d-5911-4f27-8fc6-fb1658108332",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run this command if you don't have this package installed:\n",
    "!pip install -q -U google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17196029-9295-4317-94bb-b36649f8a977",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai.types import GenerateContentConfig\n",
    "import time\n",
    "\n",
    "client = genai.Client(api_key='')\n",
    "\n",
    "llm_results_gemini = []\n",
    "count = 0\n",
    "\n",
    "for convo in msgs:\n",
    "    response = client.models.generate_content(model = 'models/gemini-2.0-flash',\n",
    "                                              contents = f\"{convo}\",\n",
    "                                              config=GenerateContentConfig(\n",
    "                                                  system_instruction=[\n",
    "                                                      \"The following is an interaction between two people.\",\n",
    "                                                      \"Determine whether malicious information has been solicited \\\n",
    "                                                      from the conversation chain. Reply with yes or no.\",\n",
    "                                                  ]\n",
    "                                              )\n",
    "                                             )\n",
    "    llm_results_gemini.append(response.text)\n",
    "    count = count + 1\n",
    "    print(f\"Prompts processed: {count}\")\n",
    "    print(\"--------\")\n",
    "    \n",
    "    # Google Gemini only processes 15 queries per minute \n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00030fca-799b-4be4-89f2-812e229f9b10",
   "metadata": {},
   "source": [
    "### OpenAI GPT-4:\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1912d9bf-c915-465b-b739-f408069e5ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key = '')\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model = \"gpt\",\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Write a one-sentence bedtime story about a unicorn.\"\n",
    "    }]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f58f126-7ce7-4f5b-981b-b703e28ca0cc",
   "metadata": {},
   "source": [
    "### Initial Proposal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442792b8-957c-4ff9-a07f-17e374adb446",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
